{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기본\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 경고 뜨지 않게 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 그래프 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
    "# plt.rcParams['font.family'] = 'AppleGothic'\n",
    "plt.rcParams['font.size'] = 16\n",
    "plt.rcParams['figure.figsize'] = 20, 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 데이터 전처리 알고리즘\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 학습용과 검증용으로 나누는 함수\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 교차 검증\n",
    "# 지표를 하나만 설정할 경우\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# 지표를 하나 이상 설정할 경우\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 모델의 최적의 하이퍼파라미터를 찾기 위한 도구\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 평가함수\n",
    "# 분류용\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# 회귀용\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 머신러닝 알고리즘 - 분류\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# 머신러닝 알고리즘 - 회귀\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "# 차원축소\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# 군집화\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.cluster import estimate_bandwidth\n",
    "\n",
    "# ARIMA (시계열 예측)\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 시간 측정을 위한 시간 모듈\n",
    "import datetime\n",
    "# 주식 정보를 읽어오기 위한 라이브러리\n",
    "from pandas_datareader import data\n",
    "\n",
    "# 형태소 백터를 생성하기 위한 라이브러리\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 형태소 백터를 학습 백터로 변환한다.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# 데이터 수집\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "\n",
    "# 한국어 형태소 분석\n",
    "from konlpy.tag import Okt, Hannanum, Kkma, Mecab, Komoran\n",
    "\n",
    "# 워드 클라우드를 위한 라이브러리\n",
    "from collections import Counter\n",
    "import pytagcloud\n",
    "from IPython.display import Image\n",
    "\n",
    "# 출력 창 청소를 위한 함수\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# 저장\n",
    "import pickle\n",
    "\n",
    "# 딥러닝\n",
    "import tensorflow as tf\n",
    "\n",
    "# 딥러닝 모델 구조를 정의하는 것\n",
    "from tensorflow.keras.models import Sequential\n",
    "# 층구조를 정의하는 것\n",
    "from tensorflow.keras.layers import Dense\n",
    "# 활성화 함수를 정의하는 것\n",
    "from tensorflow.keras.layers import Activation\n",
    "# CNN : 커널을 통해 합성곱을 구하는 것. 이미지의 특징이 두드러 지게 처리한다.\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "# MaxPooling : 불필요한 부분을 제거한다.\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "# Flatten : CNN과 MaxPooling을 하면 2차원으로 결과가 나오기 때문에 \n",
    "# 1차원으로 변환하는 것\n",
    "from tensorflow.keras.layers import Flatten\n",
    "# Dropout : 이미지나 영상은 많은 은닉층을 가지고 있어야 데이터 학습 및 탐색이\n",
    "# 가능하다. 은닉층이 많으면 과적합 될 우려가 있다. 이에 은닉층에 있는 노드를\n",
    "# 경우에 따라 사용하지 않게 하여 과적합을 예방할 수 있다.\n",
    "from tensorflow.keras.layers import Dropout\n",
    "# Embedding : 단어의 수를 줄여준다.\n",
    "from tensorflow.keras.layers import Embedding\n",
    "\n",
    "# 다중 분류를 위한 원핫 인코딩\n",
    "# 결과데이터의 종류 수 만큼 결과데이터의 컬럼을 늘리는 작업\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# 저장된 학습 모델을 읽어온다.\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# epoch마다 모델을 저장하는 함수\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "# 더이상 성능 향상이 이루어지지 않는다면 조기 중단 시킬 수 있다.\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# 문장을 잘라준다.\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# 모든 문장 데이터의 단어 데이터를 수를 동일한 수로 맞춰준다.\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# 문자열을 단어 사전으로 만들어준다.\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# 현재 프로젝트를 gpu에 할당한다.\n",
    "# 컴퓨터의 GPU는 메모리를 가지고 있다.\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "# gpu가 있다면..\n",
    "if len(gpus) > 0 :\n",
    "    try :\n",
    "        for gpu in gpus :\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e :\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장자르기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 문자열\n",
    "text = '해보지 않으면 해낼 수 없다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "원본 : 해보지 않으면 해낼 수 없다\n",
      "토큰화 : ['해보지', '않으면', '해낼', '수', '없다']\n"
     ]
    }
   ],
   "source": [
    "# 주어진 문자열을 토큰화 한다.\n",
    "result = text_to_word_sequence(text)\n",
    "print(f'원본 : {text}')\n",
    "print(f'토큰화 : {result}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단어 빈도수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = {\n",
    "    '먼저 텍스트와 각 단어를 나누어 토큰화 합니다',\n",
    "    '텍스트의 단어로 토큰화 해야 딥러닝에서 인식됩니다',\n",
    "    '토큰화 한 결과는 딥러닝에서 사용할 수 있다.'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 함수를 통해 전처리를 한다.\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('텍스트의', 1),\n",
       "             ('단어로', 1),\n",
       "             ('토큰화', 3),\n",
       "             ('해야', 1),\n",
       "             ('딥러닝에서', 2),\n",
       "             ('인식됩니다', 1),\n",
       "             ('한', 1),\n",
       "             ('결과는', 1),\n",
       "             ('사용할', 1),\n",
       "             ('수', 1),\n",
       "             ('있다', 1),\n",
       "             ('먼저', 1),\n",
       "             ('텍스트와', 1),\n",
       "             ('각', 1),\n",
       "             ('단어를', 1),\n",
       "             ('나누어', 1),\n",
       "             ('합니다', 1)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 단어의 빈도수\n",
    "# 전체 에서 어떤 단어가 몇 개씩 나왔는지....\n",
    "token.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 전체 문장의 개수\n",
    "token.document_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'해야': 1,\n",
       "             '딥러닝에서': 2,\n",
       "             '단어로': 1,\n",
       "             '인식됩니다': 1,\n",
       "             '토큰화': 3,\n",
       "             '텍스트의': 1,\n",
       "             '있다': 1,\n",
       "             '한': 1,\n",
       "             '수': 1,\n",
       "             '결과는': 1,\n",
       "             '사용할': 1,\n",
       "             '합니다': 1,\n",
       "             '나누어': 1,\n",
       "             '각': 1,\n",
       "             '단어를': 1,\n",
       "             '먼저': 1,\n",
       "             '텍스트와': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어가 몇개의 문장에 나왔었는지.\n",
    "token.word_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'토큰화': 1,\n",
       " '딥러닝에서': 2,\n",
       " '텍스트의': 3,\n",
       " '단어로': 4,\n",
       " '해야': 5,\n",
       " '인식됩니다': 6,\n",
       " '한': 7,\n",
       " '결과는': 8,\n",
       " '사용할': 9,\n",
       " '수': 10,\n",
       " '있다': 11,\n",
       " '먼저': 12,\n",
       " '텍스트와': 13,\n",
       " '각': 14,\n",
       " '단어를': 15,\n",
       " '나누어': 16,\n",
       " '합니다': 17}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 단어에 부여된 인덱스\n",
    "token.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 딥러닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 평가글\n",
    "docs = [\n",
    "    '너무 재미있네요',\n",
    "    '최고에요',\n",
    "    '참 잘 만든 영화에요',\n",
    "    '추천하고 싶은 영화입니다',\n",
    "    '한번 더 보고 싶네요',\n",
    "    '글쎄요',\n",
    "    '별로에요',\n",
    "    '생각보다 지루하네요',\n",
    "    '연기가 어색해요',\n",
    "    '재미없어요'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 데이터\n",
    "classes = np.array([1, 1, 1, 1, 1, 0, 0, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'너무': 1,\n",
       " '재미있네요': 2,\n",
       " '최고에요': 3,\n",
       " '참': 4,\n",
       " '잘': 5,\n",
       " '만든': 6,\n",
       " '영화에요': 7,\n",
       " '추천하고': 8,\n",
       " '싶은': 9,\n",
       " '영화입니다': 10,\n",
       " '한번': 11,\n",
       " '더': 12,\n",
       " '보고': 13,\n",
       " '싶네요': 14,\n",
       " '글쎄요': 15,\n",
       " '별로에요': 16,\n",
       " '생각보다': 17,\n",
       " '지루하네요': 18,\n",
       " '연기가': 19,\n",
       " '어색해요': 20,\n",
       " '재미없어요': 21}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 2],\n",
       " [3],\n",
       " [4, 5, 6, 7],\n",
       " [8, 9, 10],\n",
       " [11, 12, 13, 14],\n",
       " [15],\n",
       " [16],\n",
       " [17, 18],\n",
       " [19, 20],\n",
       " [21]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 문장을 토큰화 시킨 데이터를 단어 인덱스로 변환한다.\n",
    "X = token.texts_to_sequences(docs)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  1,  2],\n",
       "       [ 0,  0,  0,  3],\n",
       "       [ 4,  5,  6,  7],\n",
       "       [ 0,  8,  9, 10],\n",
       "       [11, 12, 13, 14],\n",
       "       [ 0,  0,  0, 15],\n",
       "       [ 0,  0,  0, 16],\n",
       "       [ 0,  0, 17, 18],\n",
       "       [ 0,  0, 19, 20],\n",
       "       [ 0,  0,  0, 21]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 리스트의 데이터의 개수를 최대 개수로 통일한다.\n",
    "padded_x = pad_sequences(X, 4)\n",
    "padded_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원핫 인코딩을 위한 다언이 개수를 파악한다.\n",
    "# 여기서 만든 단어 사전이 1부터 시작하므로 인덱스 0번째를 위해\n",
    "# 전체 단어수에 1을 더한다.\n",
    "word_size = len(token.word_index) + 1\n",
    "word_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 모델을 구성한다.\n",
    "model = Sequential()\n",
    "# Embaedding\n",
    "# 단어 값들을 원핫 인코딩을 수행한 후 결과와 상관관계 높은(빈도가 높은) 단어 \n",
    "# 단어 컬럼을 기준으로 지정한 컬럼수 만큼 선택하여 다른 컬럼의 데이터를\n",
    "# 압축하는 은닉층\n",
    "model.add(Embedding(word_size, 8, input_length=4))\n",
    "# 1차원으로 변경\n",
    "model.add(Flatten())\n",
    "\n",
    "# 출력층\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 컴파일\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 1s 1s/step - loss: 0.6952 - accuracy: 0.4000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6929 - accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6907 - accuracy: 0.5000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6885 - accuracy: 0.6000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6863 - accuracy: 0.6000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6841 - accuracy: 0.6000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6819 - accuracy: 0.7000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6797 - accuracy: 0.9000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6775 - accuracy: 0.9000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6753 - accuracy: 0.9000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6731 - accuracy: 0.9000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6710 - accuracy: 0.9000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6688 - accuracy: 0.9000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6666 - accuracy: 0.9000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6644 - accuracy: 0.9000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6622 - accuracy: 0.9000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.6600 - accuracy: 0.9000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6577 - accuracy: 0.9000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.6555 - accuracy: 0.9000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.6533 - accuracy: 0.9000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x288b82ccd90>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습\n",
    "model.fit(padded_x, classes, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 116ms/step - loss: 0.6510 - accuracy: 0.9000\n",
      "손실률 : 0.6510268449783325\n",
      "정확도 : 0.8999999761581421\n"
     ]
    }
   ],
   "source": [
    "a1 = model.evaluate(padded_x, classes)\n",
    "print(f'손실률 : {a1[0]}')\n",
    "print(f'정확도 : {a1[1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
